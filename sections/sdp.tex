%\documentclass[12pt]{article}
%\usepackage{fullpage,marginnote,caption}
%\usepackage{bbm,amsmath,amssymb,amsthm,verbatim}
%
%\input{shortcuts}
%
%\begin{document}

\section{SDP Duality}

%\input sdpbegin.tex
%\input sdpmain.tex

\subsection{Motivation}

Recall that we started with an Integer Programming formulation of the CSP - Introduce $D$ slack variables, namely $\mu_v(\ell) \in \{ 0, 1 \}, \fa \ell \in D, v \in V $ for each variable $v \in V$, with the constraint as $\sum_{\ell  \in D} \mu_v(\ell) = 1$, which is simply saying that each variable takes exactly one of the values $\ell$ in $D$. 
Similarly one introduces dummy variables for each constraint $C=(R,S)$, namely $\la_C(L) \in \{ 0, 1\} \fa L : S \rightarrow D$, where $L$ simply stands for an assignment over the variables in the scope of $C$. 
Similar to the constraint in previous case, we add the constraint $\sum_L \la_C(L) = 1$ which implies that exactly one assignment is chosen from all possible assignments for this constraint.  
Thus this is an exact formulation of CSP and hence is NP Hard. 

Then LP relaxation was motivated by relaxing the first condition for both set of dummy variables viz, $\mu_v(\ell) \in \{ 0, 1\} \rightarrow \mu_v(\ell) \in [0, 1]$ and similarly, $\la_C(L) \in [0, 1]$ but keeping the later constraints $\sum_{\ell  \in D} \mu_v(\ell) = 1 = \sum_L \la_C(L)$ intact. 
We also preserve the consistency conditions. 
This relaxation is then easily interpretable as that of an assignment of probability values to all possible combinations for variables in a constraint, and imposing conditions for consistent marginals for a variable with respect to each constraint. 

We can go a step further in restricting our search space, by introducing a second order consistency condition such that we get an SDP relaxation. 
The relaxation will be with respect to IP formulation, but tightening with respect to the LP relaxation and hence our values will be now closer to the optimal value of CSP. 
The intuition for such a relaxation can be motivated by the relaxation procedure for MAX - CUT problem which we present next. 
It is further established, that this relaxation is in-fact the optimal poly-time relaxation for any general CSP, first by assuming Unique Games Conjecture and then later in fact more generally. 

\subsection{MAX-CUT Problem - Revisited!}

MAX CUT problem can be formulated as the following integer quadratic program : 
\al{
 \max \sum_{i,j} w_{ij} \ \frac{1}{2}(1-x_ix_j) ~:~ x_i \in \{ -1, 1\}
 }
Clearly $x_i^2=1$, and $x_i=x_j\rr x_ix_j=1$ and thus contribution of that term to the sum is zero. 
Also, $x_i \neq x_j \rr x_ix_j=-1$, and thus that weight contributes to the sum. 
We can see that by associating each vertex $i$ with the variable $x_i$, we have that if an edge has both end points with same $x$-value, it is ignored in the sum, and all the edges with end points in different groups contribute weight to the sum. 
[Delorme - Poljak '90] relaxed the above program by replacing each variable $x_i$ with a unit vector $y_i$ in $\R^n$ such that its norm is 1, i.e. $y_i \in S^{n-1}$, and replacing the $x_ix_j$ by the dot product between the vectors. 
\[ \max \sum_{i,j} w_{ij} \ \frac{1}{2}(1-y_i\trans y_j) ~:~ y_i\trans y_i = 1 {\mbox {\ i.e.\ }} y_i \in S^{n-1} \]
If we define a matrix $\Sigma$ such that $$\Sigma_{ij} = y_i\trans y_j,$$ then one can easily see that for $$Y = (y_1, \cdots, y_n) \rr \Sigma = Y\trans Y \succeq 0,$$ and our program reduces to 
\[ \max_\Sigma \sum_{i,j} w_{ij} \ \frac{1}{2}(1-\Sigma_{ij}) ~:~ \Sigma \succeq 0 ~:~ \Sigma_{ii} = 1  \]
which is clearly an SDP and can be solved in polynomial time, i.e. efficiently! 
Thus we see that the SDPs did pop out for a MAX CUT relaxation.

\subsection{Basic SDP Relxation}

The SDP relaxation is similar to the LP realization but with an important generalization. 
We will have the exactly same probability distribution $\la_C$ for each constraint and the same objective function. 
However, rather than having the the marginals for the variables we will impose conditions on second order distribution of these variables. 
More precisely,
\al{
 \Pb_{L \sim \la_C}(L(v) = \ell, L(v')= \ell') = \Sigma_{\lrb{v, \ell},\lrb{v', \ell'}}, ~:~ \Sigma \succeq 0 \label{eq0}
 }
which is same as saying: 
\al{
\Pb_{x \sim \la_C} (x_i = a, x_j = b) = \Sigma'_{\lrb{i, a},\lrb{j, b}} ~:~ \Sigma' \succeq 0. 
}
Note that the two ways are equivalent, just with different notation. 
$L$ is nothing but an assignment of variables to the values, i.e., $L : V \rightarrow D$. 
We will stick to the first notation of imposing constraints over $\la_C$'s. 

The way we have defined the constraints we can see that the SDP case does cover the optimal assignment. 
If the optimal assignment was $x_i=a_i$, we can set $ \Sigma'_{\lrb{i, a},\lrb{i, a}} = 1$ and all other elements to zero, and see that $\Si' \succeq 0$. 
Thus this is indeed a relaxation.


There is a more popular but equivalent way to define these constraints, and it helps us to understand the rounding scheme that we will discuss later on. 
Note that to any positive semi-definite matrix $\Sigma$, we can associate $L$ such that, $\Si = LL\trans$, and in turn we can label $\Si$ as the covariance matrix of some joint random variables. 
Further, if we take these joint random variables as Gaussian, then we can as well make draws from them, as the first and second moments of a Gaussian  uniquely determine its distribution. 
These observations will be crucial for rounding process, but we stated them here to motivate this alternate expression of the constraints. 

We associate $\Si$ to joint real random variables $\lrb{I_v(\ell)}_{v\in V, \ell \in D}$, by calling it their covaraince matrix. Since we impose $\Si \succeq 0$, it is indeed a valid representation. 
We will also have constraints which cause these random variables to hang together with the $\la_C$'s in a gentlemanly fashion. 
These joint random variables $I_v(\ell)$ will be called as \textit{pseduoindicator random variables}. 
We emphasize that they are jointly distributed. 
One can think of them as follows: 
\textit{``There is a box, when you press a button on one side of the box i.e., when you make a draw, out comes value for each of the $|V| \cdot |D|$ random variables."}

We are ready to properly define the SDP relaxation using this random variable representation and its covariance structure.  
We denote an SDP solution by $\Sm$. 

{\bf Basic SDP for CSP$(\C)$}  
\al{
\mbox{SDPOpt}(\C) = \max_{\substack{\la_C, \Si, A}} \mbox{SDPVal}_\C(\Sm) :=\sum_{C=(R,S) \in \mathcal{C}} w_C \Pb_{L \sim \la_C} \lrbb{L(S) \mbox{\ satisfies \ } R} \text{\footnotemark} 
}
\footnotetext{A more general objective function can be defined with respect to payoff  functions $\phi_C$ associated with each constraint: $$\max_{\la_C, C\in \mathcal{C}} \sum_{C=(R,S) \in \mathcal{C}} w_C \Ex_{L \sim \la_C} \phi_C(L) = \sum_{C \in \mathcal{C}} \sum_{L \in \mathcal{L}_C} \la_C(L) \lrb{w_C\phi_C(L)}  $$}

subject to $\fa C = (R, S) \in \mathcal{C},\fa v, v' \in S, \fa \ell, \ell' \in D $
\al{
\Pb_{L \sim \la_C} (L(v) = \ell \wedge L(v') = \ell') & \os{\lrb{\ref{eq0}}}= \Sigma_{\lrb{v, \ell},\lrb{v', \ell'}} = \Ex \lrbb{I_v(\ell) \cdot I_v(\ell')} \label{csm}
}
And we also impose the conditions for valid probability distribution : 
\al{
\Pb_{L \sim \la_C} (L(v) = \ell \wedge L(v) = \ell') &= 0, \fa \ell \neq \ell' \label{eq1}\\
\sum_{\ell \in D}\Pb_{L \sim \la_C} (L(v) = \ell) &= 1 \label{eq2}
}
We also define the following constraint: (though this condition is deemed redundant by the Remark \ref{remark01} that follows, but explicitly assuming it here makes our life easy): 
\al{
\Pb_{L \sim \la_C} (L(v) = \ell ) = \Ex[I_v(\ell)] = A_{v, \ell} \label{cfm}
}

\begin{remark}\label{remark01}
If we drop the ``consistent first moment" condition (\ref{cfm}) from the Basic SDP, we get
an equivalent formulation; i.e., any solution $\mathcal{S}$ to the Basic SDP which doesn't satisfy  (\ref{cfm}) can be transformed into a solution $\mathcal{S}'$ which does satisfy  (\ref{cfm}) and has $\mbox{SDPVal}_\C(\Sm')
= \mbox{SDPVal}_\C (\Sm)$.
\end{remark}

\subsection{How is this an SDP?}

Note that for a given $C =(R, S) \in \mathcal{C}$, $\la_C$ is a probability distribution over all possible assignments $\mathcal{L}_C = \{L | L: V|_S \rightarrow D\}$. 
If we define 
\al{
\phi_C(L) = \mathbb{I}[\mbox{L satisfies R}], \fa L \in \mathcal{L}_C,
}
then 
\al{
\Pb_{L \sim \la_C} \lrbb{L(S) \mbox{\ satisfies \ } R} = \Ex_{L\sim \la_C} \phi_C(L) = \sum_{L \in \mathcal{L}_C} \la_C(L) \phi_C(L)
}
This makes the objective explicitly linear in the variables $\la_C$:
\al{
\sum_{C \in \mathcal{C}} w_C \Pb_{L \sim \la_C} \lrbb{L(S) \mbox{\ satisfies \ } R} &= \sum_{C \in \mathcal{C}} w_C \lrbb{\sum_{L \in \mathcal{L}_C} \la_C(L) \phi_C(L)}\\
&=  \sum_{C \in \mathcal{C}} \sum_{L \in \mathcal{L}_C} \la_C(L) \lrb{w_C\phi_C(L)} 
}
Next we see that 
\al{
\Sigma_{\lrb{v, \ell},\lrb{v', \ell'}} &=\Pb_{L \sim \la_C} (L(v) = \ell \wedge L(v) = \ell') \\
&= \sum_{\substack {L\in \mathcal{L}_C\\ L(v) = \ell, L(v')=\ell'} }\la_C(L)
}
is a linear constraint in the variables. 
\al{
&\sum_{\ell \in D}\Pb_{L \sim \la_C} (L(v) = \ell) = 1 \\
\lrr &\sum_{\ell \in D}  \sum_{\substack {L\in \mathcal{L}_C\\ L(v) = \ell }}\la_C(L) = 1
}
is also linear.
It is trivial to see that the last constraint is also linear (though this is redundant for analysis):
\al{
\Pb_{L \sim \la_C} (L(v) = \ell ) = \Ex[I_v(\ell)] = A_{v, \ell} \\
\lrr   \sum_{\substack {L\in \mathcal{L}_C\\ L(v) = \ell }}\la_C(L) = A_{v, \ell} 
}
Last but not the least, the way we have defined $\Si$, we have the equivalent constraint:
\al{
\Si \succeq 0
}
Thus, with a linear objective and some linear constraints, besides an SDP constraint we do have an SDP relaxation.


\subsection{Some Analysis}

\begin{remark}
The consistent first and second moment constraints imply 
\al{
\Ex \lrbb{I_v(\ell)} = \Ex \lrbb{I_v(\ell)^2}
}
\end{remark}
\begin{proof}
\al{
\Ex \lrbb{I_v(\ell)^2)} &= \Pb_{L \sim \la_C} (L(v) = \ell \wedge L(v) = \ell) \\
&= \Pb_{L \sim \la_C} (L(v) = \ell ) \\
&= \Ex[I_v(\ell)] 
}
\end{proof}
\begin{remark}
\al{
 \sum_{\ell \in D} I_v(\ell) = 1 \quad \mbox{a.s.}, ~ \fa v \in V
 }
\end{remark}
\begin{proof}
Define $J_v = \sum_{\ell \in D} I_v(\ell)$, then 
\al{
\Ex[J_v] = \sum_{\ell \in D} \Ex [I_v(\ell)] =  \sum_{\ell \in D} \Pb_{L \sim \la_C} (L(v) = \ell ) = 1
}
\al{
\Ex[J_v^2] &= \Ex \lrbb{\lrb{ \sum_{\ell \in D} \Ex [I_v(\ell)]}\lrb{ \sum_{\ell' \in D} \Ex [I_v(\ell')]}} \\
&= \sum_{\ell, \ell' \in D} \Pb_{L \sim \la_C} (L(v) = \ell \wedge L(v) = \ell') \\
&\os{\lrb{\ref{eq1}}}= \sum_{\ell \in D}\Pb_{L \sim \la_C} (L(v) = \ell) \\
&\os{\lrb{\ref{eq2}}}=1\\
&=\Ex[J]^2
}
Thus we have 
\al{
\mbox{Var}(J_v) = \Ex[J_v^2] - \Ex[J_v]^2 = 1-1 = 0 \rr J_v = J  \equiv 1 \ \mbox{a.s.}
}
\end{proof}

\subsection{Intuition for Rounding}
Next question that comes to our mind is - 
\textit{``How do we get a near optimal assignment from the solution of the relaxed problem"}. 
Note that the solution of the relaxed problem gives us a PSD matrix and probability distributions over constraints. 
Next challenge is to make a ``consistent draw" from this distribution and analyzing the expected value of CSP resulting from such draws, 
however we have already understood the intuition behind various terms and hence it is merely a computational task now.  

We concretely summarize various angles, that we discussed briefly before, to look at canonical SDP relaxations for a CSP :
\iit{

\item Pseudo-indicator random variables which satisfy the first and second moment consistency constraints. 
This perspective is arguably the best for understanding the SDP. 

\item Pseudo-indicator random variables which satisfy the second moment constraints. 
This perspective is arguably the best when constructing SDP solutions by hand.

\item Vectors $\{ y_{v, \ell} \}$ satisfying the first and second moment consistency constraints.  
This perspective is the one that's actually used computationally, on a computer. 

\item Jointly Gaussian Pseudo-indicator random variables which satisfy the consistent first and second moment constraints. 
This perspective is suited for developing``SDP Rounding algortihms". 
}
 
%\end{document}