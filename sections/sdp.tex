\section{SDP Relaxations for CSP}\maybeinclude{Raghavendra says that his SDP's are ``stronger" than other SDP's. What makes an SDP ``strong"? Is this something other people discuss?}

\subsection{Motivation}

We recall the relaxations and try to motivate our way through the various relaxations (see \cite{Ryan}) -

\begin{discussion}{\bf LP Relaxation}\\
In preparation for moving to the Basic SDP relaxation, we summarize (in words) the interpretations of $\lambda_i$ and $\mu_v$ in Basic LP.

\textit{$\fa v \in V, \mu_v(\cdot) : D \rightarrow [0, 1]$ denotes the \textit{probability distribution of variable $v$ over its range $D$}. And, $\fa C_i \in \C, \la_{C_i}(\cdot) : \Lm_{C_i} \rightarrow [0,1]$ denotes the probability distribution over all possible assignments $L \in \Lm_{C_i}$ for variables in a constraint.  
Constraint \ref{eq:canonLPConsistency} imposes conditions for consistency across distribution the $\mu_v(\cdot)$ for a variable $v$ and its marginals with respect to any constraint distribution such that the scope of that constraint contains $v$.}
\end{discussion}

We can go a step further in restricting our search space, by introducing a second order consistency condition such that we get an SDP relaxation. 
The relaxation will be with respect to IP formulation, but tightening with respect to the LP relaxation and hence our values will be now closer to the optimal value of CSP. 
The intuition for such a relaxation can be motivated by the relaxation procedure for MAX - CUT problem which we present next. 

It is further established, that this relaxation is in-fact the optimal poly-time relaxation for any general CSP, first by assuming Unique Games Conjecture \cite{Khot} and then later in fact more generally \cite{nphard}. 

\begin{example}{\bf MAX - CUT}\label{maxcut}\\
MAX CUT problem can be formulated as the following integer quadratic program : 
\al{
 \max \sum_{i,j} w_{ij} \ \frac{1}{2}(1-x_ix_j) ~:~ x_i \in \{ -1, 1\}
 }
Clearly $x_i^2=1$, and $x_i=x_j\rr x_ix_j=1$ and thus contribution of that term to the sum is zero. 
Also, $x_i \neq x_j \rr x_ix_j=-1$, and thus that weight contributes to the sum. 
We can see that by associating each vertex $i$ with the variable $x_i$, we have that if an edge has both end points with same $x$-value, it is ignored in the sum, and all the edges with end points in different groups contribute weight to the sum. 
\cite{delormecombinatorial} relaxed the above program by replacing each variable $x_i$ with a unit vector $y_i$ in $\R^n$ such that its norm is 1, i.e. $y_i \in S^{n-1}$, and replacing the $x_ix_j$ by the dot product between the vectors. 
\[ \max \sum_{i,j} w_{ij} \ \frac{1}{2}(1-y_i\trans y_j) ~:~ y_i\trans y_i = 1 {\mbox {\ i.e.\ }} y_i \in S^{n-1} \]
If we define a matrix $\Sigma$ such that $$\Sigma_{ij} = y_i\trans y_j,$$ then one can easily see that for $$Y = (y_1, \cdots, y_n) \rr \Sigma = Y\trans Y \succeq 0,$$ and our program reduces to 
\[ \max_\Sigma \sum_{i,j} w_{ij} \ \frac{1}{2}(1-\Sigma_{ij}) ~:~ \Sigma \succeq 0 ~:~ \Sigma_{ii} = 1  \]
which is clearly an SDP and can be solved in polynomial time, i.e. efficiently! 
Thus we see that the SDPs did pop out for a MAX CUT relaxation.
\end{example}

\subsection{Basic SDP Relaxation}

The SDP relaxation is similar to the LP realization but with an important generalization. 
We will have the exactly same probability distribution $\la_{C_i}$ for each constraint and the same objective function. 
However, rather than having the the marginals for the variables we will impose conditions on second order distribution of these variables. 
More precisely,
\al{
 \Pb_{L \sim \la_{C_i}}(L(v) = \ell, L(v')= \ell') = \Sigma_{\lrb{v, \ell},\lrb{v', \ell'}}, ~:~ \Sigma \succeq 0 \label{eq0}
 } 

The way we have defined the constraints we can see that the SDP case does cover the optimal assignment. 
If the optimal assignment was $v=\ell_v$, we can set $ \Sigma'_{\lrb{v, \ell_v},\lrb{v', \ell_{v'}}} = 1$ and all other elements to zero, and see that $\Si' \succeq 0$. 
Thus this is indeed a relaxation.


There is a more popular but equivalent way to define these constraints, and it helps us to understand the rounding scheme that we will discuss later on. 
Note that to any positive semi-definite matrix $\Sigma$, we can associate $L$ such that, $\Si = LL\trans$, and in turn we can label $\Si$ as the covariance matrix of some joint random variables. 
Further, if we take these joint random variables as Gaussian, then we can as well make draws from them, as the first and second moments of a Gaussian  uniquely determine its distribution. 
These observations will be crucial for rounding process, but we stated them here to motivate this alternate expression of the constraints. 

We associate $\Si$ to joint real random variables $\lrb{I_v(\ell)}_{v\in V, \ell \in D}$, by calling it their covariance matrix. Since we impose $\Si \succeq 0$, it is indeed a valid representation. 
We will also have constraints which cause these random variables to hang together with the $\la_{C_i}$'s in a gentlemanly fashion. 
These joint random variables $I_v(\ell)$ will be called as \textit{pseduoindicator random variables}. 
We emphasize that they are jointly distributed. 
%One can think of them as follows: 
%\textit{``There is a box, when you press a button on one side of the box i.e., when you make a draw, out comes value for each of the $|V| \cdot |D|$ random variables."}

We are ready to properly define the SDP relaxation using this random variable representation and its covariance structure.  
We denote an SDP solution by $\Sm$. Also, $R_i(L(S_i))$ is the indicator if the local assignment $L$ on $S_i$ satisfies the constraint $R_i$. This also gives: 
\al{
\Ex_{L \sim \la_{C_i}} \lrb{R_i(L(S_i))} = \Pb_{L \sim \la_{C_i}} (L:S_i \rightarrow D^{|S_i|} {\rm \ satisfies \ } R_i)
}
\begin{definition}{\label{def1}}
{\bf Basic SDP for CSP$(\C)$}  
\al{
\mbox{SDPOpt}(\C) &= \max_{\substack{\la_{C_i}, \Si, A}} \mbox{SDPVal}_\C(\Sm) := \Ex _{C_i \sim W} \lrbb{\Ex_{L \sim \la_{C_i}} \lrb{R_i(L(S_i))}} %\text{\footnotemark} \\
%&=  \max_{\substack{\la_{C_i}, \Si, A}} \sum_{C_i \in \C}w_{C_i} \Ex_{L \sim \la_{C_i}} \lrb{R_i(L(S_i))}
}
%\footnotetext{A more general objective function can be defined with respect to payoff  functions $\phi_C$ associated with each constraint: $$\max_{\la_C, C\in \mathcal{C}} \sum_{C=(R,S) \in \mathcal{C}} w_C \Ex_{L \sim \la_C} \phi_C(L) = \sum_{C \in \mathcal{C}} \sum_{L \in \mathcal{L}_C} \la_C(L) \lrb{w_C\phi_C(L)}  $$}

subject to the constraint $\fa C_i = (R_i, S_i) \in \mathcal{C},\fa v, v' \in S, \fa \ell, \ell' \in D $
\al{
\Pb_{L \sim \la_{C_i}} (L(v) = \ell, L(v') = \ell')  &\os{\lrb{\ref{eq0}}}= \Sigma_{\lrb{v, \ell},\lrb{v', \ell'}} = \Ex \lrbb{I_v(\ell) \cdot I_v(\ell')} \label{csm}\\
\Si &\succeq 0.
}
And we also impose the constraints for valid probability distribution : 
\al{
\sum_{L \in \Lm_i} \la_{C_i}(L) &= 1, \fa C_i \in \C\\
\sum_{\ell \in D}\Pb_{L \sim \la_{C_i}} (L(v) = \ell) &= 1, \fa v\in V, \fa C_i \in \C \label{eq2}\\
\Pb_{L \sim \la_{C_i}} (L(v) = \ell, L(v) = \ell') &= 0, \fa \ell \neq \ell', \fa v\in V, \fa C_i \in \C \label{eq1}
}
We also define the following constraint:\footnote{This condition is deemed redundant by Remark \ref{remark01}, but explicitly assuming it here makes our life easy.} 
\al{
\Pb_{L \sim \la_{C_i}} (L(v) = \ell ) = \Ex[I_v(\ell)] = A_{v, \ell} \label{cfm}
}
\end{definition}

\subsection{Is Definition \ref{def1} an SDP?}

Note that for a given $C_i =(R_i, S_i) \in \mathcal{C}$, $\la_{C_i}$ is a probability distribution over all possible assignments $\mathcal{L}_{C_i} = \{L: S \rightarrow D\}$. Clearly,  
\al{
\Ex_{L \sim \la_{C_i}} \lrb{R_i(L(S_i))} &= \Pb_{L \sim \la_{C_i}} (L:S_i \rightarrow D^{|S_i|} {\rm \ satisfies \ } R_i) \\
&= \sum_{L \in \mathcal{L}_{C_i}} \la_{C_i}(L) \mathbb{I} \lrbb{ L {\rm \ satisfies \ }R _i }
}
is linear in $\la_{C_i}$'s.
This makes the objective explicitly linear in the variables $\la_{C_i}$'s:
\al{
\Ex _{C_i \sim W} \lrbb{\Ex_{L \sim \la_{C_i}} \lrb{R_i(L(S_i))}} &= \sum_{C_i \in \C}w_{C_i} \Ex_{L \sim \la_{C_i}} \lrb{R_i(L(S_i))} \\
&=  \sum_{C_i \in \mathcal{C}} \sum_{L \in \mathcal{L}_{C_i}} \la_{C_i}(L) \lrb{w_{C_i}\mathbb{I} \lrbb{ L {\rm \ satisfies \ }R _i }} 
}
Next we see that 
\al{
\Sigma_{\lrb{v, \ell},\lrb{v', \ell'}} &=\Pb_{L \sim \la_{C_i}} (L(v) = \ell \wedge L(v) = \ell') \\
&= \sum_{\substack {L\in \mathcal{L}_{C_i}\\ L(v) = \ell, L(v')=\ell'} }\la_{C_i}(L)
}
is a linear constraint in the variables. Next, 
\al{
&&\sum_{\ell \in D}\Pb_{L \sim \la_{C_i}} (L(v) = \ell) &= 1 \\
&\lrr &\sum_{\ell \in D}  \sum_{\substack {L\in \mathcal{L}_{C_i}\\ L(v) = \ell }}\la_{C_i}(L) &= 1
}
is also linear. 
It is trivial to see that the last constraint is also linear:
\al{
&&\Pb_{L \sim \la_{C_i}} (L(v) = \ell ) &= \Ex[I_v(\ell)] = A_{v, \ell} \\
&\lrr   &\sum_{\substack {L\in \mathcal{L}_{C_i}\\ L(v) = \ell }}\la_{C_i}(L) &= A_{v, \ell} 
}
Last but not the least, we also have, 
\al{
\Si \succeq 0
}
which is a linear matrix inequality. Thus, with a linear objective and some linear constraints, besides an SDP constraint we do have an SDP relaxation.

\subsection{Some Analysis}
\begin{remark}\label{remark01}
If we drop the ``consistent first moment" condition (\ref{cfm}) from the Basic SDP, we get
an equivalent formulation; i.e., any solution $\mathcal{S}$ to the Basic SDP which doesn't satisfy  (\ref{cfm}) can be transformed into a solution $\mathcal{S}'$ which does satisfy  (\ref{cfm}) and has $\mbox{SDPVal}_\C(\Sm')
= \mbox{SDPVal}_\C (\Sm)$.
\end{remark}

\begin{remark}
The consistent first and second moment constraints imply 
\al{
\Ex \lrbb{I_v(\ell)} = \Ex \lrbb{I_v(\ell)^2}
}
\end{remark}
\begin{proof}
\al{
\Ex \lrbb{I_v(\ell)^2)} &= \Pb_{L \sim \la_C} (L(v) = \ell \wedge L(v) = \ell) \\
&= \Pb_{L \sim \la_C} (L(v) = \ell ) \\
&= \Ex[I_v(\ell)] 
}
\end{proof}
\begin{remark}
\al{
 \sum_{\ell \in D} I_v(\ell) = 1 \quad \mbox{a.s.}, ~ \fa v \in V
 }
\end{remark}
\begin{proof}
Define $J_v = \sum_{\ell \in D} I_v(\ell)$, then 
\al{
\Ex[J_v] = \sum_{\ell \in D} \Ex [I_v(\ell)] =  \sum_{\ell \in D} \Pb_{L \sim \la_C} (L(v) = \ell ) = 1
}
\al{
\Ex[J_v^2] &= \Ex \lrbb{\lrb{ \sum_{\ell \in D} \Ex [I_v(\ell)]}\lrb{ \sum_{\ell' \in D} \Ex [I_v(\ell')]}} \\
&= \sum_{\ell, \ell' \in D} \Pb_{L \sim \la_C} (L(v) = \ell \wedge L(v) = \ell') \\
&\os{\lrb{\ref{eq1}}}= \sum_{\ell \in D}\Pb_{L \sim \la_C} (L(v) = \ell) \\
&\os{\lrb{\ref{eq2}}}=1\\
&=\Ex[J]^2
}
Thus we have 
\al{
\mbox{Var}(J_v) = \Ex[J_v^2] - \Ex[J_v]^2 = 1-1 = 0 \rr J_v = J  \equiv 1 \ \mbox{a.s.}
}
\end{proof}

