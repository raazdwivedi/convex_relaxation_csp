\section{Linear Programming for CSP Approximation}
The goal of this section is to present a ``canonical" LP relaxation and rounding scheme for Constraint Satisfaction Problems. Both the relaxation and rounding scheme are valid for arbitrary CSP's, although performance guarantees are only proven in limited cases.  A non-exhaustive list of CSP-specific LP relaxations is included at the end of this section.

\subsection{An Integer Program : Towards the Canonical LP [CITE]}\label{subsec:ip}
Let $\mathcal{C} = (V,C,W)$ be a CSP over a domain $D$ of size $q$.

Earlier, we defined an \textit{assignment of variables} as a function $F : V \to D$. Critically, the domain of $F$ is the entire set $V$. When this is the case, we could call $F$ a \textit{full assignment}. It is reasonable (and as we will see, helpful!) to consider $F$ as being built from many \textit{local assignments} $L : S \to D$ where $S \subset V$. For a constraint $C_i = (R_i,S_i) \in \mathcal{C}$, we will be interested in the local assignment $L : S_i \to D$. Where before we could write $R_i(F(S_i))$ as the value of the constraint under an assignment, we write $R_i(L)$ when it is given that $L$ is a local assignment for constraint $C_i$.

Now consider an \textit{Integer}-Linear Program over the following variables:
\begin{itemize}
\item $\mu_v[\ell] \in \{0,1\}$ is an indicator that variable $v \in V$ takes value $\ell \in D$
\item $\lambda_i[L] \in  \{0,1\}$ is an indicator that \textit{local assignment} $L$ is used for constraint $C_i$
\end{itemize}
From these definitions, it is clear that for fixed $v$, we need one and only one $\mu_v[\ell]$ to be equal to 1. Encode this constraint as 
\begin{equation*}
\sum_{\ell \in D} \mu_v[\ell] = 1.
\end{equation*}

Now we define $\mathcal{L}_i$ as the set of all possible local assignments for the variables in constraint $C_i$'s scope. We note that the size of $\mathcal{L}_i$ is exponential in $t = ar(R_i)$ (in fact, it's exactly $|\mathcal{L}_i| = q^t$). This is one of the key reasons why maximum arity is a \textit{fixed parameter} for all $\mathcal{C} \in \text{CSP}(\Gamma)$.

Since two $L_1$,$L_2$ in $ \mathcal{L}_i$ are mutually exclusive, we likewise need one and only one of $\lambda_i[L]$ equal to 1 for fixed $i$.
\begin{equation*}
\sum_{L \in \mathcal{L}_i} \lambda_i[L] = 1 
\end{equation*}
To be consistent across $\lambda$ and $\mu$, we need one more constraint.
\begin{equation*}
\mu_v[\ell] = \sum_{\substack{ L \in \mathcal{L}_i \\ L(v) = \ell }} \lambda_i[L] 
\end{equation*}
Finally, we use the objective
\begin{equation*}
\max \sum_{i : C_i \in C} \sum_{L \in \mathcal{L}_i}   w_i\lambda_i[L] R_i(L).
\end{equation*}

We get the corresponding LP simply by relaxing $\mu_v[\ell] \in \{0,1\}$ to $\mu_v[\ell] \in [0,1]$ and $\lambda_i[L] \in \{0,1\}$ to $\lambda_i[L] \in [0,1]$.
\subsection{The Canonical Linear Program}

We define the linear program below, then address its probabilistic interpretation and equivalent representations.
\begin{definition}\textbf{Basic LP} \\
Let $\mathcal{C} = (V,C,W)$ be a CSP over domain $D$. The Basic LP for $\mathcal{C}$ is
\begin{alignat}{2}
\max ~&~ \sum_{i : C_i \in C} \sum_{L \in \mathcal{L}_i}   w_i\lambda_i[L] R_i(L) & \\
s.t. ~ & ~ \sum_{\ell \in D} \mu_v[\ell] = 1 & \forall v \in V  \label{eq:canonLPmuSum} \\
     ~ & ~ \sum_{L \in \mathcal{L}_j} \lambda_j[L] = 1  & \forall i : C_i \in C \label{eq:canonLPlambdaSum} \\
     ~ & ~ \sum_{\substack{ L \in \mathcal{L}_i \\ L(v) = \ell }} \lambda_i[L] = \mu_v[\ell]  & \forall v \in V, \ell \in D, i : C_i \in C \label{eq:canonLPConsistency} \\
     ~ & ~ 0 \leq \lambda_i[L] \leq 1  & \forall  i : C_i \in C, L \in \mathcal{L}_i  \label{eq:canonLPlambdaNonNeg} \\
     ~ & ~ 0 \leq \mu_v[\ell] \leq 1 & v \in V, \ell \in D \label{eq:canonLPmuNonNeg}
\end{alignat}
\end{definition}

Note that between constraints \ref{eq:canonLPmuNonNeg} and \ref{eq:canonLPmuSum}, a solution $\mu^*_v[\ell]$ to the above LP defines a probability distribution (over $\ell \in D$) of assignments for $v$. That is, if we wanted to randomly generate an assignment $\hat{\ell}_v$ for variable $v$, then we could simply state $\hat{\ell}_v = \ell$ with probability $\mu^*_v[\ell]$. Given this interpretation, we write
\begin{equation}
\mu_v[\ell] = \mathbb{P}\left( \hat{\ell}_v = \ell \right) \quad \text{ where } \quad \hat{\ell}_v \sim \mu_v 
\end{equation}

A similar interpretation holds for $\lambda$; between constraints \ref{eq:canonLPlambdaNonNeg} and \ref{eq:canonLPlambdaSum}, a solution $\lambda^*_i[L]$ defines a probability distribution (over $L \in \mathcal{L}_i$) of possible local assignments for constraint $C_i$. That is, if we wanted to randomly select a local  assignment $\hat{L}_i$ for constraint $C_i$, we could state $\hat{L}_i = L$ with probability $\lambda^*_i[L]$. Given this interpretation, we write
\begin{equation}
\lambda_i[L] = \mathbb{P}\left( \hat{L}_i = L \right) \quad \text{ where } \quad \hat{L}_i \sim \lambda_i 
\end{equation}
  
These distributions are tied together by constraint \ref{eq:canonLPConsistency}. In the probabilistic terms established above, constraint \ref{eq:canonLPConsistency} reads
\begin{equation}
\mathbb{P}\left( \hat{\ell}_v = \ell \right) = \sum_{\substack{L \in \mathcal{L}_i \\ L(v) = \ell}} \mathbb{P}\left( \hat{L}_i = L \right) \qquad \forall v \in V, \ell \in D, i : C_i \in C
\end{equation}
Since the events  $\{\hat{L}_i = L_1 \}$ and $\{\hat{L}_i = L_2 \}$ are mutually exclusive, the right hand side can be rewritten to give the following. This simply states that the probability that a variable $v$ takes on value $\ell$ is the same whether you consider the distribution as being defined from $\mu_v$ or $\lambda_i$, for any $i : C_i \in C$. We refer to this constraint as a \textit{first moment consistency constraint}.
\begin{equation} 
\mathbb{P}\left( \hat{\ell}_v = \ell \right) = \mathbb{P}\left( \bigcup_{L : L(v) =  \ell} \left\{\hat{L}_i = L\right\} \right) \qquad \forall v \in V, \ell \in D, i : C_i \in C
\end{equation}

As we remarked in Section \ref{sec:introToCSP}, we can write the objective function of a CSP in the probabilistic terms $\max_{F : V \to D} \mathbb{E}\left[ R_i(F(S_i)) \right]$. With slight modifications (to reflect the fact that our decision variables are now \textit{local} rather than global assignments), we can make a similar statement. Namely, let $C_i \in C$ be randomly drawn from distribution $W$ and let local assignment $L$ be randomly drawn from distribution $\lambda_i$. Then 
\[
	OPT_{LP} = \max_{\lambda_{\mathcal{C}}} \underset{C_i \sim W}{\Ex} \left[ \underset{L \sim \lambda_{i}}{\Ex} \left[ R_i( L (S_i )) \right] \right]
\]
where $R_i( L (S_i ))$ is an indicator whether assignment $L$ for scope $S_i$ satisfies the relation given by $R_i$.

\begin{thm}
	Basic LP is a relaxation for the problem CSP$(\Gamma)$.\todo{unsure}{for binary CSP's or arbitrary CSP's?}
\end{thm}
\begin{proof}
	Given an optimal solution for an instance of the problem CSP($\Gamma$), assign $\mu_v^t$ to be $1$ if the variable $x_v$ takes value $t$ in the optimal solution to the CSP($\Gamma$) instance and $0$ otherwise.
	Furthermore, for each $C_i \in \mathcal{C}$ set $\lambda_{C_i}[y]$ equal to $1$ if $y$ is the local assignment for the scope $S_i$ corresponding to the optimal solution to the CSP and $0$ otherwise. 
	It follows that this is a feasible LP solution. 
	Hence, $OPT(\mathcal{C}) \le OPT_{LP}(\mathcal{C})$.
\end{proof}

Having obtained a valid LP relaxation for any CSP problem, the question is now how to use this relaxation to generate good feasible solutions to the CSP. 
A key technique to generate CSP solutions is rounding the LP solution.

\subsection{A Rounding Scheme for the Canonical LP Relaxation}
Various techniques have been developed to extract good CSP solutions from Basic LP.
A common technique to generate CSP solutions with a performance guarantee is randomized rounding.
This technique uses the information available from the LP, typically by using the optimal solution as a probability distribution, to randomly round the each variable to a value in its domain $D$.
To showcase a potential rounding technique, let us consider the problem of Max $k$-SAT.

\subsubsection{Randomized Rounding scheme for LP relaxation of Max k-SAT}
Consider the full assignment $F$ as a vector of random variables. For each variable $v \in V$, let the random variable be given by:
\[
	F(v) = \begin{cases}
	1 & \text{with probability } \mu^*_v(1)\\
	0 & \text{with probability } \mu^*_v(0)
	\end{cases}
\]
where $\mu^*_v(\ell)$ is the value of variable $\mu_v(\ell)$ in an optimal solution to the LP.
Since the sum over $\ell \in D = \{0,1\}$ of  $\mu_v(\ell)$ is $1$, and since $\mu_v(\ell)$ is non-negative, the above is a valid definition for a random variable. 
Furthermore, note that $F(1), \dots, F(n)$ is a feasible solution to the Max k-SAT problem.

The expected objective value of this randomized assignment is given by:
\[
	\underset{F}{\Ex}\left[ \underset{C_i \sim W}{\mathbb{P}}[ F \text{ satisfies } C_i] \right]
\]
Since we can swap the order of expectation (where $\mathbb{P}$ is an expectation over an indicator), this is equivalent to
\[
\underset{C_i \sim W}{\mathbb{E}}\left[ \underset{F}{\mathbb{P}}[ F \text{ satisfies } C_i] \right]
\]
This swap allows us to consider each $C_i \in \mathcal{C}$ separately. 
Therefore, a single constraint $C_i \in \mathcal{C}$ can be considered.

Observe that since the constraints are in disjunctive form, each literal needs to evaluate to false for the constraint to be false. 
In other words, there is a unique falsifying assignment $b_{C_i}$ that makes constraint $C_i$ false. 
For example, if $C_i = x_1 \vee x_2 \vee \bar{x}_3$, then the unique falsifying assignment is given by $x_1 = 0$, $x_2 = 0$, and $x_3 = 1$. 
Considering the definition of $F(v)$ and the independence of $F(i)$ and $F(j)$ for $i \neq j$, it then follows that 
\begin{equation}
		\underset{F}{\mathbb{P}}[ F \text{ satisfies } C_i] = 1 - \prod_{v \in S_i} \mu_v(b_{C_i}(v)) \label{eq:objectiveRounding}
\end{equation}
where $b_{C_i}(v)$ is the value of variable $v \in S_i$ in the falsifying assignment. 
In particular, the probability of not satisfying the constraint in the above example is $ 1- \mu_1^*(0) \mu_2^*(0) \mu_3^*(1)$.

Having identified the objective value of the randomized solution, the question is whether any guarantees can be made on the quality of the solution compared to the LP. 
This can be done, by relating $F$ to the distribution over local assignments.
\begin{align}
	p_{C_i} &:= \underset{L \sim \lambda_{C_i}^*}{\mathbb{P}} [L \text{ satisfies } C_i] \nonumber\\
			&=  \underset{L \sim \lambda_{C_i}^*}{\mathbb{P}}\left[ \bigcup_{v \in S} \left\{ L(v) \neq b_{C_i}(v) \right\} \right] \nonumber\\
			&\le \sum_{v \in  S } \underset{L \sim \lambda_{C_i}^*}{\mathbb{P}} \left[  \left\{ L(v) \neq b_{C_i}(v) \right\} \right] \nonumber\\
			&= \sum_{v \in S} \left( 1 - \mu_v^*(b_{C_i}(v)) \right) \label{eq:objectiveLP}
\end{align}
This last step follows from the first-order consistency constraints:
\[
	\underset{L \sim \lambda_{C_i} }{\mathbb{P}}[ L(v) = \ell] = \underset{L \sim \mu_v }{\mathbb{P}}[ L = \ell]
\]

It remains to relate the LP objective value to the expected objective of the rounding procedure. 
Note that the former (see \eqref{eq:objectiveLP}) is an arithmetic mean (AM) whereas the latter (see \eqref{eq:objectiveRounding}) is an geometric mean (GM). 
This suggests the use of the inequality of arithmetic and geometric means.
\begin{align*}
		\underset{F}{\mathbb{P}}[ F \text{ satisfies } C_i] &= 1 - \prod_{v \in S_i} \mu_v(b_{C_i}(v))\\
		&= 1 - \underset{v \in S}{GM}(\mu_{v}^*(b_c(v)))^{|S|}\\
		&\ge 1 - \underset{v \in S}{AM}(\mu_{v}^*(b_c(v)))^{|S|}\\
		&= 1 - \left( 1  - \underset{v \in S}{AM}(1 -\mu_{v}^*(b_c(v))) \right)^{|S|}\\
		&\ge 1 - \left( 1  - \frac{p_{C_i}}{|S|}\right)^{|S|}\\
\end{align*}

If $|S| \leftarrow \infty$, then the ratio approaches $1 - \frac{1}{e}$. 
Therefore, the objective value of the rounding scheme
\begin{align*}
	\underset{F}{\Ex} \left[ \text{Val}_{\mathcal{C}}[F]\right] &= \underset{F}{\Ex}\left[ \underset{C_i \sim W}{\mathbb{P}}[ F \text{ satisfies } C_i] \right]\\
	&=  \underset{C_i \sim W}{\mathbb{E}}\left[ \underset{F}{\mathbb{P}}[ F \text{ satisfies } C_i] \right]\\
	&\ge \left( 1 - \frac{1}{e} \right) \underset{C_i \sim W}{\Ex} [ p_{C_i} ]\\
	&= \left( 1 - \frac{1}{e} \right) \text{OPT}_{LP}(\mathcal{C})\\
	&\ge \left( 1 - \frac{1}{e} \right) \text{OPT}(\mathcal{C})\\
\end{align*}

Hence, we conclude with the following theorem:
\begin{thm}
	Randomized rounding is a $\left( ( 1 - 1/e)\beta, \beta \right)$-approximation for Max-SAT for any $\beta$.
\end{thm}
\subsection{CSP-Specific LP Relaxations and Rounding Schemes}
Note the following specific relaxations
\begin{itemize}
\item this thing
\item this thing as well
\item also this thing
\end{itemize}

\iffalse
\subsection{An Integer-Linear Program}
For a given CSP $\mathcal{C} = (V,C,W)$ over domain $D$, consider an ILP over the following variables

 $z_c$ to denote whether constraint $c \in \mathcal{C}$ is satisfied and binary variables $\mu_v^t$ to denote whether variable $v \in V$ takes value $t \in D$. 
Then, a CSP can be denoted as the following integer program.
\begin{alignat}{3}
	\max \quad & \sum_{C_i \in \mathcal{C}}w_{C_i} z_{C_i} & \label{eq:lp_weak}\\
	\text{s.t.} \quad & z_{C_i} \le R_{i}( \mu_{S_i} ) & \quad C_i = (R_i, S_i) \in \mathcal{C} \nonumber \\
	& \sum_{t \in D} \mu_v^t = 1 & \quad v \in V \nonumber\\
	& z_{C_i} \in \{0,1\} & \quad {C_i} \in \mathcal{C} \nonumber\\
	&	\mu_v^t \in \{0,1\}	& \quad v \in  V \nonumber
\end{alignat}
where $\mu_{S_i}$ are the variables corresponding to the CSP variables $x_{S_i}$ in the scope of constraint ${C_i} \in \mathcal{C}$ and $R_i$ is a linear formulation\footnote{Assuming that such a formulation is indeed possible.} of the operator $R_i \in \Gamma$ corresponding to constraint $C_i$. 
That is $R_i$ evaluates to $1$ if the constraint is satisfied and $0$ otherwise. 
Note that if the constraint ${C_i}$ is satisfied, then $z_{C_i} = 1$ and $w_{C_i}$ is contributed to the objective function, and if the constraint is not satisfied, then $z_{C_i}$ is forced to be zero. 

A convex relaxation can be obtain by relaxing the integrality constraints, and require that $\mu_v^t, z_{C_i} \in [ 0,1]$. 
Then, all $\mu_v^t$ for $t \in D$ can be considered as a probability distribution over the possible values $x_v$ can take.
However, this LP relaxation is usually weak, and it provides no information about possible assignments. In particular, consider the case of Max-Cut. 
Then, following the formulation in \eqref{eq:lp_weak}, the Max-Cut problem can be formulated as:
\begin{alignat}{3}
\max \quad & \sum_{[u,v]\in E}w_{uv} z_{uv} & \label{eq:maxCut_weak}\\
\text{s.t.} \quad & z_{uv} \le x_u + x_v & \quad [u,v] \in E \label{eq:maxCut_low} \\
& z_{uv} \le 2 - ( x_u + x_v ) & \quad [u,v] \in E \label{eq:maxCut_upp}\\
& z_{uv} \in \{0,1\} & \quad [u,v] \in E \nonumber\\
&	x_v \in \{0,1\}	& \quad v \in  V \nonumber
\end{alignat}
where we replace $\mu_{u}^0$ and $\mu_u^1$ with a single variable $x_u$. 
Constraints \eqref{eq:maxCut_low} and \eqref{eq:maxCut_upp} ensure that $w_{uv}$ contributes to the cut if and only if $x_u$ and $x_v$ have different values.

Let us consider the LP formulation for the formulation given in \eqref{eq:maxCut_weak}. 
By the definition of CSP, the weights sum up to $1$. 
Hence, the maximum achievable objective value for the LP relaxation is $1$. 
Consider the solution $z_{uv}$ equal to $1$ for all $[u,v] \in E$ (i.e. the set of constraints $C$) and $x_v = \frac{1}{2}$. 
Observe that this solution is feasible for the LP relaxation, since each of the constraints. Furthermore, the solution is optimal since it achieves the maximum achievable value of $1$. 
However, this LP relaxation does not provide information about the best value of the variable $x_u$. 
Therefore, a more involved LP formulation is considered.
\fi